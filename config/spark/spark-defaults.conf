# ========================================
# Iceberg Catalog Configuration
# ========================================
# Create a catalog named "iceberg" (reference as iceberg.database.table)
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog

# Use JDBC (PostgreSQL) as the catalog backend
spark.sql.catalog.iceberg.type=jdbc
spark.sql.catalog.iceberg.uri=jdbc:postgresql://postgres:5432/airflow
spark.sql.catalog.iceberg.jdbc.user=airflow
spark.sql.catalog.iceberg.jdbc.password=airflow

# S3 warehouse location for data files
spark.sql.catalog.iceberg.warehouse=s3://iceberg-warehouse/

# Use S3FileIO for reading/writing
spark.sql.catalog.iceberg.io-impl=org.apache.iceberg.aws.s3.S3FileIO

# ========================================
# S3/LocalStack Configuration
# ========================================
# LocalStack endpoint (change to real AWS for production)
spark.hadoop.fs.s3a.endpoint=http://localstack:4566

# AWS credentials (use real credentials in production)
spark.hadoop.fs.s3a.access.key=test
spark.hadoop.fs.s3a.secret.key=test

# Path-style access (required for LocalStack)
spark.hadoop.fs.s3a.path.style.access=true

# S3A FileSystem implementation
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# ========================================
# Iceberg Spark Extensions
# ========================================
# Enable Iceberg-specific SQL syntax (e.g., time travel)
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions